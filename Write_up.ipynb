{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Behavioral Cloning** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Behavioral Cloning Project**\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "* Use the simulator to collect data of good driving behavior\n",
    "* Build, a convolution neural network in Keras that predicts steering angles from images\n",
    "* Train and validate the model with a training and validation set\n",
    "* Test that the model successfully drives around track one without leaving the road\n",
    "* Summarize the results with a written report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files Submitted & Code Quality\n",
    "\n",
    "#### 1. Submission includes all required files and can be used to run the simulator in autonomous mode\n",
    "\n",
    "My project includes the following files:\n",
    "* model.py containing the script to create and train the model\n",
    "* drive.py for driving the car in autonomous mode\n",
    "* model.h5 containing a trained convolution neural network \n",
    "* writeup_report.md or writeup_report.pdf summarizing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Submission includes functional code\n",
    "Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing \n",
    "```sh\n",
    "python drive.py model.h5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Submission code is usable and readable\n",
    "\n",
    "The model.py file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model, and it contains comments to explain how the code works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture and Training Strategy\n",
    "\n",
    "#### 1. An appropriate model architecture has been employed\n",
    "\n",
    "The network follow NVIDIA network, consists of 9 layers, including a \n",
    "   1. normalization layer resulting in features ranging from values -1.0 to 1.0\n",
    "   2. Five convolutional and maxpooling layers\n",
    "   3. and 5 fully-connected layers\n",
    "  \n",
    "the input image has the shape of (64, 64, 3). It goes to first convolution, the output of firts convolution will be (32, 32, 24). Activation function of relu wont change the size. Then it goes to MaxPooling2D which decrease the size of the input to (31, 31, 24).\n",
    "\n",
    "in layer2:\n",
    "the input image has the shape of (31, 31, 24). It goes to second convolution, the output of this convolution will be (16, 16, 36). Activation function of relu wont change the size. Then it goes to MaxPooling2D which decrease the size of the input to (15, 15, 36).\n",
    "\n",
    "in layer3:\n",
    "the input image has the shape of (15, 15, 36). It goes to third convolution, the output of this convolution will be (8, 8, 48). Activation function of relu wont change the size. Then it goes to MaxPooling2D which decrease the size of the input to (7, 7, 48).\n",
    "\n",
    "in layer4:\n",
    "the input image has the shape of (7, 7, 48). It goes to fourth convolution, the output of this convolution will be (7, 7, 64). Activation function of relu wont change the size. Then it goes to MaxPooling2D which decrease the size of the input to (6, 6, 64).\n",
    "\n",
    "in layer5:\n",
    "the input image has the shape of (6, 6, 64). It goes to fifth convolution, the output of this convolution will be (6, 6, 64). Activation function of relu wont change the size. Then it goes to MaxPooling2D which decrease the size of the input to (5, 5, 64).\n",
    "\n",
    "Then the input of (5, 5, 64) will be flattened to 1600 (64*5*5) fiters, it goes to first fully connected layer with 1164 depth. Activation function of relu wont change the size. In second fully connected layer, the depth will be decreased to 100 and in third one it decreased to 50, in fourth one it decreased to 10, and the last one which is the regression it just gives us the final predicted steering angle.\n",
    "\n",
    "\n",
    "More implementation details:\n",
    "\n",
    "1. Adam optimizer with learning rate of 1e-4.\n",
    "2. optimization method for minimizing loss:  minimize mean_squared_error.\n",
    "3. ModelCheckpoint to save model weights after each epoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Attempts to reduce overfitting in the model\n",
    "\n",
    "The model contains data augmentation to reduce overfitting. The original data was composed of 24108 images including center, left and right images with angles of the center, left, and right cameras. after augmentation training data includes 28929 samples and validation includes 7233 samples. Also, I applied dropout in every fully connected layer.\n",
    "\n",
    "The model was tested by running it through the simulator and ensuring that the vehicle could stay on the track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Model parameter tuning\n",
    "\n",
    "The model used an adam optimizer with learning rate of 10^-4. I also tried to choose ReduceLROnPlateau function to monitor the validation loss so that if it doesn't change withing 5 time-steps it attempts to reduce it by a factor of 0.8. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Appropriate training data\n",
    "\n",
    "Training data was chosen to keep the vehicle driving on the road. I used the data provided by the udacity. I used the center, left, and right cameras and augmented to have a three times larger dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture and Training Strategy\n",
    "\n",
    "#### 1. Solution Design Approach\n",
    "\n",
    "The overall strategy for deriving a model architecture was to first do data augmentation and then designing the neural network based on NVIDIA model from paper that udacity provided.\n",
    "\n",
    "NVIDIA model is appropriate because it has enough number of convolutional layers to detect the required features and enough fully connected layers to analyze them.\n",
    "\n",
    "I split my image and steering angle data into a training (80%) and validation set (20%). \n",
    "\n",
    "At final step I tried to run the simulator to see how well the car was driving on trained model. There were some few  spots where the car fell off the track. Those were certainly the right angle turns which the car could not do it right. Inorder to improve these cases, I added the correction angle to right and left angle measurements. \n",
    "\n",
    "At the end of the process, the vehicle is able to drive autonomously around the track without leaving the road."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Final Model Architecture\n",
    "\n",
    "The final model architecture consisted of a convolution neural network with the following layers a\n",
    "\n",
    "\n",
    "     Layer (type)                 Output Shape              Param #   \n",
    "    =================================================================\n",
    "    batch_normalization_2 (Batch (None, 64, 64, 3)         12        \n",
    "    _________________________________________________________________\n",
    "    conv2d_6 (Conv2D)            (None, 32, 32, 24)        1824      \n",
    "    _________________________________________________________________\n",
    "    activation_7 (Activation)    (None, 32, 32, 24)        0         \n",
    "    _________________________________________________________________\n",
    "    max_pooling2d_6 (MaxPooling2 (None, 31, 31, 24)        0         \n",
    "    _________________________________________________________________\n",
    "    conv2d_7 (Conv2D)            (None, 16, 16, 36)        21636     \n",
    "    _________________________________________________________________\n",
    "    activation_8 (Activation)    (None, 16, 16, 36)        0         \n",
    "    _________________________________________________________________\n",
    "    max_pooling2d_7 (MaxPooling2 (None, 15, 15, 36)        0         \n",
    "    _________________________________________________________________\n",
    "    conv2d_8 (Conv2D)            (None, 8, 8, 48)          43248     \n",
    "    _________________________________________________________________\n",
    "    activation_9 (Activation)    (None, 8, 8, 48)          0         \n",
    "    _________________________________________________________________\n",
    "    max_pooling2d_8 (MaxPooling2 (None, 7, 7, 48)          0         \n",
    "    _________________________________________________________________\n",
    "    conv2d_9 (Conv2D)            (None, 7, 7, 64)          27712     \n",
    "    _________________________________________________________________\n",
    "    activation_10 (Activation)   (None, 7, 7, 64)          0         \n",
    "    _________________________________________________________________\n",
    "    max_pooling2d_9 (MaxPooling2 (None, 6, 6, 64)          0         \n",
    "    _________________________________________________________________\n",
    "    conv2d_10 (Conv2D)           (None, 6, 6, 64)          36928     \n",
    "    _________________________________________________________________\n",
    "    activation_11 (Activation)   (None, 6, 6, 64)          0         \n",
    "    _________________________________________________________________\n",
    "    max_pooling2d_10 (MaxPooling (None, 5, 5, 64)          0         \n",
    "    _________________________________________________________________\n",
    "    flatten_2 (Flatten)          (None, 1600)              0         \n",
    "    _________________________________________________________________\n",
    "    dense_2 (Dense)              (None, 1164)              1863564   \n",
    "    _________________________________________________________________\n",
    "    activation_12 (Activation)   (None, 1164)              0         \n",
    "    _________________________________________________________________\n",
    "    dropout_1 (Dropout)          (None, 1164)              0         \n",
    "    _________________________________________________________________\n",
    "    dense_3 (Dense)              (None, 100)               116500    \n",
    "    _________________________________________________________________\n",
    "    activation_13 (Activation)   (None, 100)               0         \n",
    "    _________________________________________________________________\n",
    "    dropout_2 (Dropout)          (None, 100)               0         \n",
    "    _________________________________________________________________\n",
    "    dense_4 (Dense)              (None, 50)                5050      \n",
    "    _________________________________________________________________\n",
    "    activation_14 (Activation)   (None, 50)                0         \n",
    "    _________________________________________________________________\n",
    "    dropout_3 (Dropout)          (None, 50)                0         \n",
    "    _________________________________________________________________\n",
    "    dense_5 (Dense)              (None, 10)                510       \n",
    "    _________________________________________________________________\n",
    "    activation_15 (Activation)   (None, 10)                0         \n",
    "    _________________________________________________________________\n",
    "    dropout_4 (Dropout)          (None, 10)                0         \n",
    "    _________________________________________________________________\n",
    "    dense_6 (Dense)              (None, 1)                 11        \n",
    "    =================================================================\n",
    "    Total params: 2,116,995\n",
    "    Trainable params: 2,116,989\n",
    "    Non-trainable params: 6\n",
    "    \n",
    "    \n",
    "![](./NVIDIA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Creation of the Training Set & Training Process\n",
    "Here are the random sample pictures from each camera:\n",
    "![](./sample_image.png)\n",
    "\n",
    "I randomly flipped images to augment data and generate more images from all three cameras. About 40 pixels from top and 20 pixels from bottom are useless, so I cropped those parts of each images. \n",
    "\n",
    "I randomly shuffled the data set and spilit data to 20% of the data into a validation set to help me know whether the model is overfitting or underfitting and the rest for training set.\n",
    "I used an adam optimizer so that manually training the learning rate wasn't necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Final Driving Model:\n",
    "I conducted my test for behavioral cloning model one track one using similator provided by udacity. I changed the setting to Resolution at 640 * 480 and Graphics quality at \"SIMPLE\". The saved video is in my submitted file. During training, I realized that perfomance of my model changed everytime. Also, performance was affected by quality of simulator as well. Also, I updated the drive.py file to make the image transformations to the input image before feeding it to the model: crop top and bottom of image and resize it to 64x64x3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
